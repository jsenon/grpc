= Kubernetes
Julien SENON <julien.senon@gmail.com>
:revnumber: 0.1
:revdate: 2017-10-15
:sectanchors:
:seclinks:
:toc: left
:docdate:
:icons: font
:page-layout: docs

=== Architecture for CRF project

.Architecture overview
image::k8sdesign.png[k8sdesign]


For the moment only one Kubernetes cluster per AWS account form dev to staging environments

==== AZ

Kubernetes cluster is split between three AZ in one region for high availability purpose.

[NOTE]
Use Replica with minimum two where three replicas is the best to do split between each AZ.


==== Subnet

We have defined two different subnet per AZ in the three AZ. +
One private subnet with nodes and master and one public subnet with Nat gateway, Bastion and ELB

==== NAT

Three NAT gateway is exposed on public subnet in each AZ of the region `eu-central-1`. They are used to provide external link from containers. Traffic is form *Nodes to External* only.

==== Routes

By default:

* Nodes default gateway is the NAT gateway available for its region (EGRESS)
* Elastic Load Balancer defautl gateway is the Internet Gateway
* Bastion default gateway is the Internet Gateway

[IMPORTANT]
Nodes are not available from external world, they are only accessible through Bastion

==== ASG

We create 4 AWS Auto Scaling Groups:

* one master ASG for region eu-central-1a
* one master ASG for region eu-central-1b
* one master ASG for region eu-central-1c
* one bastion ASG
* one nodes ASG

=== In production


==== Replicas

As describe in architecture compute nodes are split between different region. In order to have High Availability, number of replicas should always be a minimum of two, where three is more recommended.

==== Rediness and Liveness Probes

Readiness should be configure in order to handle traffic only when container succesfully started:

.readiness.yaml
[source,yaml]
----
containers:
- name: mycontainer
  image: myimage
  readinessProbe:
    httpGet:
      # Path to probe; should be cheap, but representative of typical behavior
      path: /.well-known/health
      port: 8080
    timeoutSeconds: 1
----

Liveveness probes should be configure in order to detetct and remedy broken states

.liveness.yaml
[source,yaml]
----
containers:
- name: liveness
image: myimage
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
    httpHeaders:
    - name: X-Custom-Header
      value: Awesome
  initialDelaySeconds: 3
  periodSeconds: 3
----

=== Ressource Requests and limits


Requests are used by the *scheduler* to find a node that has free resources to take the pod. +
Limits are *maximum ressource* allowed of containers

Always configure ressource request for CPU and Memory in order to have autoscaler functionalities. If request is not available a new node will be provisionned

.requests.yaml
[source,yaml]
----
containers:
  - name: mycontainer
    image: myimage
    resources:
      requests:
        cpu: 100m     # 100 millicores
        memory: 200Mi # 200 MiB
----

[TIP]
 If you don’t specify a limit for a Container, the Container could use all of the memory or cpu available on the Node where it is running.

[TIP]
If the Container is running in a namespace that has a default memory or cpu limit, the Container is automatically assigned the default limit.

A Container can exceed its memory request if the Node has memory available. But a Container is not allowed to use more than its memory limit.

.limits.yaml
[source,yaml]
----
containers:
  - name: mycontainer
    image: myimage
    resources:
      limits:
        cpu: 100m     # 100 millicores
        memory: 200Mi # 200 MiB
----

If a Container allocates more memory than its limit, the Container becomes a candidate for termination. If the Container continues to consume memory beyond its limit, the Container is terminated. If a terminated Container is restartable, the kubelet will restart it, as with any other type of runtime failure.

CPU resources are measured in virtual cores or more commonly in "millicores" (e.g. 500m denoting 50% of a vCPU). +
Memory resources are measured in Bytes and the usual suffixes can be used, e.g. 500Mi denoting 500 Mebibyte. +
See https://en.wikipedia.org/wiki/Mebibyte[Mebibyte]

[IMPORTANT]
Configure request in order to have cluster autoscaler.

=== Labels

Labels are key/value pairs that are attached to Kubernetes objects, such as pods. +
Labels can be attached to objects at creation time and subsequently added and modified at any time.

[CAUTION]
Check with all.

Following labels should be defined in our context:

* application
* version
* release
* stage
* tribe/squad
* owner

Example
.labels.yaml
[source,yaml]
----
metadata:
  labels:
    application: myapp
    version: "v1"
    release: "r2"
    stage: production
    tribe: mytribe
    owner: me
----

Label is used by `service` or `replicationcontroller` with a lable selector

.selector.yaml
[source,yaml]
----
selector:
    application: myapp
----

Newer resources, such as `Job`, `Deployment`, `Replica Set`, and `Daemon Set`, support set-based requirements as well.

.selector.yaml
[source,yaml]
----
selector:
  matchLabels:
    component: myapp
----

=== Volumes

==== Ephemeral Volume

A Container’s file system lives only as long as the Container does, so when a Container terminates and restarts, changes to the filesystem are lost.

Define volume in your definition as bellow.

.ephemeral.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: myapp
    volumeMounts:
    - name: myapp-storage
      mountPath: /data/myapp
  volumes:
  - name: myapp-storage
    emptyDir: {}
----

==== Persistent Volume

. Define a Persitent Volume
. Define a Volume claim to bound pod to an existing volume
. Create a pod that use Volume claim

*Persistent volume*

In our case it's created dynamically with volumeclaim based on StorageClass


*Volume Claim*

.volumeclaim.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myapp-data
  annotations:
    volume.beta.kubernetes.io/storage-class: myclass
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 1Gi
----

*Pod volume request*

.myapp.yaml
[source,yaml]
----
volumeMounts:
        - name: myapp-data
          mountPath: /myapp_data
----

==== Storage class

Storage class AWS example

.storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: myclass
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zones: eu-central-1a, eu-central-1b, eu-central-1c
  iopsPerGB: "10" # only fo io1
----

*type* could be io1, gp2, sc1, st1. Default is gp2
See http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html[AWS docs]

=== Cheat Sheet

Get Cluster info:
[source,zsh]
kubectl cluster-info

Get deployment
[source,zsh]
kubectl get deployment

Get service
[source,zsh]
kubectl get services

Get pods
[source,zsh]
kubectl get pods

Describe a pod
[source,zsh]
kubectl describe pod POD_NAME

Retrieve Log
[source,zsh]
kubectl logs -f POD_NAME [-c CONTAINER_NAME]

Open sh prompt inside container
[source,zsh]
kubectl exec -it POD_NAME CONTAINER_NAME sh

Exec command inside container
[source,zsh]
kubectl exec -it POD_NAME CONTAINER_NAME cat /etc/myconf.conf

Create ressource:
[source,zsh]
kubectl apply -f myressource.yaml

Scale ressource:
[source,zsh]
kubectl scale deploy DEPLOY_NAME --replicas=3

[TIP]
By default command is performed over default namespace, use `-n namespace` to change it.

[cols="1,1", options="header"]
.Ressources Listing
|===
|Full Name
|Short Name

|configmaps
|cm

|deployments
|deploy

|endpoints
|ep

|events
|ev

|horizontalpodautoscalers
|hpa

|ingress
|ing

|jobs
|

|limitranges
|limits

|namespaces
|ns

|nodes
|no

|statefulsets
|

|pods
|po

|replicasets
|rs

|cronjob
|

|secrets
|

|serviceaccount
|sa

|services
|svc

|thirdpartyresources [will be deprecated]
|
|===


=== Config Map

ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable.

.configmap.yaml
[source,yaml]
----
----

.myapp.yaml
[source,yaml]
----
----

=== Pod Initialization

An init container is defined in POD definition, it will run once before starting container.

.configmap.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: myapp-init
spec:
  containers:
  - name: myapp
    image: maypp
    ports:
    - containerPort: 80
    volumeMounts:
    - name: myapp-data
      mountPath: /data/myapp
  # These containers are run during pod initialization
  initContainers:
  - name: install
    image: busybox
    command:
    - wget
    - "-O"
    - "/data/index.html"
    - http://kubernetes.io
    volumeMounts:
    - name: myapp-data
      mountPath: "/data"
  dnsPolicy: Default
  volumes:
  - name: myapp-data
    emptyDir: {}
----

Init container will launch command `wget` before starting container `myapp` it will retrieve an `index.hmtl` and store it in volume `myapp-data`. init container is detroyed and `myapp-data` will be mounted to `myapp` container on `/data/myapp`. `index.html` is available in `/data/myapp/index.html`

=== Secrets



=== Job Cleaner

Kubernetes jobs are not cleaned up by default and completed pods are never deleted. Running jobs frequently (like every few minutes) quickly thrashes the Kubernetes API server with unnecessary pod resources. +
Purpose of Job cleaner is to run `Cronjob` every hour to cleanup completed jobs. +
See https://github.com/hjacobs/kube-job-cleaner[Project in github]


=== Todo

* [ ] Selector matchExpression
* [ ] Validation of label
* [ ] Quality of Service for Pods
* [ ] Secret
* [ ] Config map



